\hfill
\subsubsection{Probabilistic Principal Component Analysis (PPCA)}
%[REFERENCE: http://www.robots.ox.ac.uk/~cvrg/hilary2006/ppca.pdf]

%SEE how it its implemented in Julia
PCA is not based on a probability model. However, we can extend PCA by determining the PCs of a data set by maximum-likelihood estimation of parameters in a latent variable model as described in [REFERENCE].

We can use a Gaussian latent variable model %which is closely related to statistical factor analysis  [MAYBE EXPLAIN FACTOR ANALYSIS FIRST?] 
to derive a probabilistic formulation of PCA. The PCs result from maximum-likelihood parameter estimates which can be computed by the usual eigendecomposition of the sample covariance matrix and subsequently incorporated in the model [REFERENCE THESIS ABOVE]. 

Alternatively, it is natural to apply an iterative, and computationally rather efficient Expectation-Maximization (EM) algorithm to estimate the principal subspace, ie effecting PCA.

%PROPERTIES, ADVANTAGES???

%PRO:
Intuitively it seems appealing to formulate PCA in a probabilsitic way, as having a likelihood measures enables us to compare it to other probabilitic methods. Also, it allows statistical testing and Bayesian methods to be applied. 
%A probabilistic formulation is intuitively appealing, as the definition of a likelihood measure enables comparison with other probabilistic techniques, while facilitating statistical testing and permitting the application of Bayesian methods. 
Moreover, PPCA has some practical advantages and can extend the scope of classic PCA [REFERENCE]:
%However, another motivation is that probabilistic PCA has additional practical advantages and can extend the scope of conventional PCA:
\begin{itemize}
	\item  Multiple PCA models may be combined as a probabilistic mixture and PCA projections could be obtained even when some data values are missing.
	\item Besides its application to DR, PPCA can be used as a general Gaussian density model. This has the advantage that maximum-likelihood estimates for the parameters associated with the covariance matrix can be efficiently computed from the PCs and could be applied to eg classification. 
\end{itemize}
%
%INSERT PROJECTION AND RECONSTRUCTION FIGURE


%%----------------------------------------------------------------------------------------------------------------------------------------
\hfill
\subsubsection{Independent Component Analysis (ICA)}
[UNFINISHED]
%TODO
- tries to decompose a multivariate signal into additive subcomponents;
we assume that the subcomponents are (1) non-Gaussian signals and that (2) they are statistically independent from each other
- often used in signal processing; eg sound is usually a signal that is composed of additive signals from several sources at each time \( t \)
- special case of blind source separation;
- common example application is the \textit{cocktail party problem} where one tries to filter out an underlying speech signal, eg a person's speech, in a noisy room. 
% Simplified bt assuming no time delays or echoes. Since the filtered and delayed signal is a copy of a dependent component, the statistical interdependence assumption is not violated.

%The ICA algorithm gives good results if the two above-mentioned assumptions are met and additionally the following three effects of mixing source signals are met: %SOURCE???!!
%1. Independence:
%2. Normality:
%3. Complexity:

[INSERT TABLE THAT COMPARES PCA VS ICA] 

%JADE algorithm


%.ICA:
- finds independent components (ie factors, latent variables) by maximising the statistical independence of the estimated components
- there are many ways to define a proxy for independence, eg minimisation of mutual information or maximation of non-gaussianity. The first one uses the Kullback-Leibler Divergence and maximum entropy; the second one is motivated by the central limit theorem %, and uses kurtosis and negentropy. %WHATS THAT???!!
PCA:
...

% -- show results
preprocessing steps in order to simplify/ reduce the complexity of the problem to the actual iterative algorithm
- uses centering (subtract the mean to create a zero mean signal)
- whitening (usually with eigenvalue decomposition using PCA or SVD); ensures that all dimensions are treated equally \textit{a priori} before the algorithm is applied. 
- actual dimensionality reduction 

[IMPLEMENTATION JADE ALGORITHM FOR ICA -- PSEUDOCODE]
%% WRITE DOWN MATHEMATICALLY


%\subsection{Non-linear methods}

%%----------------------------------------------------------------------------------------------------------------------------------------
\hfill
\subsubsection{Kernel PCA (KPCA)}
[UNFINISHED]

Linear methods can often be powerful, but do not detect non-linear structures in the data.
Kernel PCA tries to generalise linear methods like PCA for non-linear data.
It uses the kernel trick to transform non-linear data to a feature space were samples may be linearly separable.

MOTIVATION: NOT LINEARLY SEPARABLE DATA...., to generalise PCA through kernel based techniques. does not rely on structure of the manifold on which the data may possibly reside. 

KERNEL TRICK. COOL!!..: WE NEVER ACTUALLY WORK DIRECTLY IN THE FEATURE SPACE BUT ONLY VIA DOT PRODUCT...


[MAYBE PROVIDE EXAMPLE BUT ON OTHER DATA SET?]

[NOT VERY USEFUL AS LINEAR DR PERFORMS WELL ENOUGH, EG WE HAVE NO CIRCULAR DATA SET.] 


%%----------------------------------------------------------------------------------------------------------------------------------------
\hfill
\subsubsection{Other approaches}

Other classical linear approach that have been examined included Multidimensional Scaling (MDS), Linear Discriminant Analysis (LDA) and Factor Analysis. 
%MDS takes a matrix of pair-wise distances between all points, and computes a position for each point. 
%Fisher's Linear Discriminant Analysis (LDA) takes into account the class of observaions, ie there are observations, supervised, returns axes that maximizes class separability (same constraint that axes are also orthogonal)


%INSERT OVERVIEW OF ALL METHODS IN MULTIVARIATESTATS
