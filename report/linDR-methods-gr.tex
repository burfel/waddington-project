%%-----------------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Linear methods and Kernel PCA}

%%----------------------------------------------------------------------------------------------------------------------------------------
\subsubsection{Principal Component Analysis (PCA)}

%TALK: PCA STEP BY STEP: http://sebastianraschka.com/Articles/2014_pca_step_by_step.html

Mathematically speaking, all PCA is is a basis transformation with an optional subsequent projection. It is linear in the sense that it only takes linear combinations of the old basis (features, here: genes) to form the new basis. 

In other words, PCA finds the principal components (PCs) that explain the data "best", ie containing most information which is usually quantified by the total variance captured by the basis elements. The PCs are, therefore, the directions of maximum variance in the high-dimensional data. Projecting the data onto this smaller dimensional subspace (spanned by the PCs) will reduce the dimensionality of the originial feature space and still retain most of the information (variance). \\
We will see that the new basis can be obained by performing an eigendecomposition on the initial data set, and in fact equals the eigenvectors.

%%% TO BE PUT SOMEWHERE ELSE MAYBE
%The first step of PCA also helps us to identify patterns in the data, by detecting correlation between variables; this can also be used for a simple feature selection.
%We can argue that only if a strong correlation between variables exists, the attempt to reduce the dimensionality makes sense.

%FOR SLIDES:
%Summary of PCA approach:
%- standardise data
%- obtain eigenvectors and eigenvalues from the covariance matrix (or correlation matrix --- WHATS DIFFERENCE) or perform singular value decomposition (see approach SVD / generalPCA.jl)
%- sort eigenvalues in descending order and choose k eigenvectors that correspond to the k largest eigenvalues where k is \# dimensions of the feature subspace (k <= d)
%- construct the projection matrix W from the selected k eigenvectors
%- transform the original dataset X via W to obtain k-dimensional feature subspace Y.

%There are 3 simple steps to PCA:
PCA can be summed up in three simple steps:
\begin{enumerate}
	\item Eigendecomposition: Computing eigenvectors and eigenvalues
	\item Selecting principal components
	\item Projection onto the new feature space
\end{enumerate}
%\newline
%
%%-----------------------------------------------------------------------
\textbf{Eigendecomposition: Computing eigenvectors and eigenvalues}\\
Eigenvectors and eigenvalues of a covariance (or correlation) matrix are at the very centre of PCA. The eigenvectors (PCs) determine the direction of the new feature space; the eigenvalues determine their magnitude. 
We can obtain the eigenvectors and eigenvalues from the covariance matrix or correlation matrix or perform singular value decomposition (see approach SVD / generalPCA.jl)

For the reader who is unfamiliar to eigendecomposition, we insert a brief mathematical interlude on this central result in linear algebra. To learn about how to find eigenvalues and its corresponding eigenvectors, we refer to [REFERENCE].

Let \( A \in \mathbb{R}^{d,d}\) be a matrix. A non-zero vector \( u \) is an eigenvector of \( A \) with a corresponding eigenvlaue \( \lambda \) if 
\begin{equation}
	A u = \lambda u .
\end{equation}

It can be shown with basic linear algebra results that every (non-singular) diagonalisable matrix \( A \) can be factorised into a canonical form, whereby \( A \) is represented in terms of its eigenvalues and eigenvectors.
Note, the matrix \( A \) is diagonalisable if and only if the algebraic multiplicity equals the geometric multiplicity of each eigenvalues, ie \( A \) has \( d \) distinct eigenvalues.

This result follows from the \textit{Spectral Decomposition Theorem}. 
\begin{theorem}\label{spectralthm}
	If \( A \in \mathbb{R}^{d,d}\) is a symmetric matrix of rank \( k \), then there exists an orthonormal basis of \( \mathbb{R}^d \), namely \( u_{1}, u_{2}, ..., u_{d} \), such that each \( u_{i} \) is an eigenvector of \( A \). Furthermore, \( A \) can be written as \( A = \sum_{i=1}^{d} \lambda_{i} u_{i} u_{i}^T \), where each \( \lambda_{i} \) is the eigenvalue corresponding to the eigenvector \( u_{i} \). Equivalently, this can be written as \( A = U D U^T \), where the columns of \( U \) are the vectors \( u_{1}, u_{2}, ..., u_{d} \), and \( D \) is a diagonal matrix with \( D_{i,i} = \lambda_{i} \) and for \( i \neq j \) we have \( D_{i,j} = 0 \). Finally, the number of \( \lambda_{i} \neq 0 \) determines the rank of the matrix; the eigenvectors which correspond to the non-zero eigenvalues span the range of \( A \), and the eigenvectors which correpsond to zero eigenvalues span the null space of \( A \).
\end{theorem}

Assuming that the reader now knows the meaning and power of eigendecomposition, we now go back to PCA.\\
% plot.ly documentation
%%--------------------------------------
\textbf{a. PCA with covariance matrix}:
Here, before starting, one usually standardises the data. %% WHY, COMPARE!
The classic approach of PCA is to perform the eigendecomposition of the covariance matrix \( \Sigma \), which is a \( d \times d \) matrix whose elements represent the covariance between two features calculated by 
\[
\sigma_{jk} = \frac{1}{n-1} \sum_{i=1}^{N} (x_{ij} - \bar{x}_{j}) (x_{ik} - \bar{x_{k}})
\]
which equals the matrix equation
\[
\Sigma = \frac{1}{n-1} ( (X - \bar{x})^{\text{T}} (X - \bar{x}) )
\]
where \( \bar{x }\) is the mean vector \( \bar{x}= \sum_{k=1}^{n} x_{i}  \).

The mean vector is a \(d\)-dimensional vector where each value in this vector represents the sample mean of a feature column in the data set.\\
%%---------------------------------------
\textbf{b. PCA with correlation matrix}: 
Instead of the covariance matrix, one can use the correlation matrix. 
Since the correlation matrix can be understood as the normalised covariance matrix, the eigendecomposition on the covariance matrix yields the same results (eigenvectors and eigenvalues) as one on the correlation matrix in case of standardised input data. 
In fact, we could show that we yield the same results for the following three approaches:
\begin{itemize}
\item eigendecomposition of the covariance matrix after standardising the data
\item eigendecomposition of the correlation matrix
\item eigendecomposition of the correlation matrix afer standardising the data.
\end{itemize}
For more computational efficiency, one often uses SVD for the eigendecomposition (see section \ref{svd}).
%%%
%%%
\newline
%
%%----------------------------------------------------------------------------------
\textbf{Selecting the principal components}\\
The eigenvectors form the new basis, ie the directions of the new axes and all have the same unit length 1. To decide onto which eigenvectors we want to project down to, ie which eigenvectors "to keep", we look at the corresponding eigenvalues. The eigenvectors with the highest eigenvalues capture the most distribution or information of the data and the ones that should be kept. 
For this purpose, we sort the eigenvalues from highest to lowest in order to later choose the top \( k \) eigenvectors, where \( k \) is the number of dimensions of the feature subspace and \( k \leq d \). \\
%% INSERT PLOT explained variance, 
%% PRODUCE PLOT cumulative explained variance
A frequently asked question is: What is the size of \( k \) that represents the data well?

% from plotly documentation

For that, we construct the projection matrix \( P \) by "concatenating" the just computed \( d \) eigenvectors. Each of those eigenvectors comes with an eigenvalue (we will call them \textit{eigenpairs}) which can be interpreted as the "length" or "magnitude" of the corresponding eigenvector. 

If some eigenvalues have a significantly larger magnitude than others, the reduction of the data set via PCA onto a lower-dimensional subspace by dropping "less informative" eigenpairs seems reasonable.

%INSERT STATISTICS
%Therefore, we rank the eigenvalues from the highest to lowest in order to choose the top \( k \) eigenvectors. 
By computing the "explained variance", ie amount of information each PC (eigenvector) captures, on our data set, we see that most of the information, approximately 21.85 \% to be precise, can be explained by the first PC. The subsequent components each bear information between approximately 8.36\% and less than 0.129\%. Together, the first two PCs account for more than 30\% (30.22\%) of the information. The third PC explains less than a third of what the first components; as a result, three PCs capture 36.78\% of the total variance. We examined that 75 dimensions can be seen as benchmark as the first 75 PCs are enough to retrieve almost all (99.06\%) of the variance of the data. For details, we refer to figure [REFERENCE]. \\


% Based on that, the user can decide how much PC they want to have.
To come back to the question above, we leave that up to the user that can choose either a fixed number of PCs spanning the new feature space or a percentage of explained variance that the PCs represent in total based on the computation of explained variance.
%%
%%
\newline
%
%%------------------------------------------------------------------------------------
\textbf{Projection onto the new feature space}\\
%- transform the original dataset X via W to obtain k-dimensional feature subspace Y.
To project the data set onto the new feature subspace, we take top \( k \) eigenvectors of the just constructed projection matrix \( P \) which we will call \( P_{k} \).
Last but not least, we transform the original data set \( X \) via \( P_{k} \) into \( Y \) in the new \(k\)-dimensional feature subspace by computing \( Y = X \times P_{k} \).

[MAYBE INSERT PSEUDOCODE HERE]

For a more rigorous mathematical definition we refer to [REFERENCE SHALEV-SHWARTZ BOOK].


%% WHATS DIFFERENCE BETWEEN PCA FROM PACKAGE AND GENERALISEDPCA? JULIA PCA BASED ON EIGENDECOMPOSITION, NOT ON SVD?

%INSERT PLOT
%Figure [ADD] [REFERENCE] shows a graphical sketch of a PCA transformation in two dimensions.



%%----------------------------------------------------------------------------------------------------------------------------------------
\hfill 
\subsubsection{Singular Value Decomposition (SVD)}\label{svd}

%https://stats.stackexchange.com/questions/107533/how-to-use-svd-for-dimensionality-reduction-to-reduce-the-number-of-columns-fea

A bit less intuitive than the eigendecomposition of the covariance or the correlation matrix, is the SVD.\\
Let \( A \in \mathbb{R}^{m,n} \) be a matrix of rank \( r \). If \( m \neq n\), the eigenvalue decomposition in \ref{spectralthm} cannot be applied. However, we can perform another decomposition on \( A \), the so-called \textit{Singular Value Decomposition} (SVD) which makes use of the \textit{SVD Theorem} [REFERENCE].
\begin{theorem}\label{svdthm}
	Let \( A \in \mathbb{R}^{m,n} \) with rank \( r \). Then \( A = UDV^T\), where \( D \) is a \( r \times r \) matrix with non-zero singular values of \( A \) and the columns of \( U, V \) are orthonormal left and right singular vectors of \( A \). Furthermore, for all \( i \), \( D_{i,i}^{2} \) is an eigenvalue of \( A^T A\), the \( i \)th column of \( V \) is the corresponding eigenvector of \( A^T A\) and the \( i \)th column of \( U \) is the corresponding eigenvector of \( A A^T\).
\end{theorem}

% results in same eprojection directions and feature vectors
% 
%EXPLAIN MORE ON SVD HERE
For the theory, we refer to [REFERENCE]. In \texttt{generalPCA.jl}, another PCA method based on SVD is implemented.\\

As a proof-of-concept example, we apply our PCA method to the given sample data set [SEE SECTION ....].
Figure [ADD] shows the explained variance of the principle components, computed from the singular values of the data matrix (rows correspond to features, ie genes, and columns corespond to the samples). 
%Additionally, table [ADD] shows the more details on the variances (observation and residual variances), depending on how many output dimensions, ie principal components (PCs), we choose. 75 dimensions can be seen as benchmark as the first 75 PCs are enough to retrieve 99.06\% of the data. 

As discussed before, the first three components account for more than 30\% of the total variance of the data; 75 dimensions are enough to recontruct the data almost completely. The latter might be interesting from a data compression point of view; as we are only interested in the visualisation, we only look at two or three dimensions.\\

Figure [........] shows the projection onto the first two principal components and on the first three principal components 
Additionally, we try to reconstruct the data after reducing the dimensions. 

We can see that the reconstruction of the original data works rather well; the third PC does not seem to add much more information.
However, as the initial basis (dimensions, or genes) were not sorted according to variance, it might be more reasonable to reconstruct the dimensions, ie genes, with the highest variance. From the exploratory data analysis section, we know that these are the genes \textit{Gdf3}, \textit{Fgf4} and \textit{Cldn6}. The reconstruction onto these dimensions is found in figure [......]
% DISCUSS WHAT RECONSTRUCTION MEANS, INITAL AXES...

As it is hard to capture how the new dimensions (PCs) lie in space, we try to visualise each of them by their unique "barcode", see figure [REFERENCE]. As each PC, ie new basis, is a linear combination of the initial basis, we can think of this barcode as a function of the original dimensions:
\begin{equation}
	PC_{i}  := f(x_{1}, x_{2}, ..., x_{N}) = \sum_{i=1}^{N} w_{i} \cdot x_{i}
	%w_{1} \cdot x_{1} + w_{2} \cdot x_{2} + ... + w_{N} \cdot x_{i} 
\end{equation}
where \( x_{i} \) denotes the elements of the inital basis, ie in our case genes and \( w_{i} \) are the coefficients or weights of each initial component. In our case \( N = 96 \), as we have 96 in the input data set. From the barcode, we identified \textit{Fgf4} as highest contributor, ie gene with the highest absolute value of the \( w_{i} \) for the first PC, \textit{Cldn6} for the second PC, and \textit{Cdh2} for the third PC. \\
We noitce that both \textit{Fgf4} as well as \textit{Cdh2} are amongst the top 5 genes with the highest variance. 
Plotting the contribution of all genes summed over all PCs showed that \textit{Tubb3} contributed the most and \textit{MBP} the least; \textit{MBP} in fact shows no expression at all. We note, that this is in line with the initial exploratory analysis as we noticed that both \textit{MBP} and \textit{Smarca4} have zero mean and zero variance; we could have discarded both genes at that point.
However, there is no significant variance which is in line of what we expected.


%BIOLOGICAL INTERPRETATION.


[INSERT FIGURE]

For further studies it might be interesting to segment the data according to cell types or time points and analyse them separately to detect pattern. Subsequent k-means clustering was shown to identify three different groups in the data set [REFERENCE PAPER].
Also one could anal

%ADD PLOTS (PROJECTION)

%SVD from https://docs.julialang.org/en/stable/stdlib/linalg/ 
% TODO: have a look how exactly they do it.

%%-----------------END-----GROUP REPORT---------------------------
%%----------------------------------------------------------------------------------------------------------------------------------------